# ğŸ› ï¸ tiny-transformer-tr
*Build, train & deploy a **small-footprint Turkish GPT** model entirely from first-principles. 100Â % reproducible on GoogleÂ Colab or any CUDAâ€‘enabled Windows workstation.*

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/<YOUR_GH_USER>/tiny-transformer-tr/blob/main/notebook.ipynb)  
[![ğŸ¤— Model Card](https://img.shields.io/badge/HF%20Model-tiny--transformer--tr-orange?logo=huggingface)](https://huggingface.co/<YOUR_HF_USER>/tiny-transformer-tr)  
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)  
[![CI](https://github.com/<YOUR_GH_USER>/tiny-transformer-tr/actions/workflows/ci.yml/badge.svg)](https://github.com/<YOUR_GH_USER>/tiny-transformer-tr/actions)

---

## 0. Why does this repo exist?
> *Because technical interviewers worldwide value engineers who can **explain a transformer layer by layer**Â â€” not just call `from_pretrained()`.*

* **Engineering challenge** â€“ we build every stage (tokenizerÂ â†’ data loaderÂ â†’ modelÂ â†’ evaluation) from scratch so you can answer deepâ€‘dive questions.  
* **Scientific rigor** â€“ experiments logged with *Weights &Â Biases*; scripts deterministic; seeds pinned.  
* **Production mindset** â€“ ready for HF Hub, Docker, ONNX &Â Triton inference.  

---

## 1Â Â·Â TL;DR
| Item | Spec |
|------|------|
| Corpus | Turkish WikipediaÂ 2022â€‘03 (â‰ˆÂ 2Â GB after cleaning) |
| Tokenizer | Byteâ€‘LevelÂ BPEÂ Â·Â 30â€¯k vocab |
| Model | GPTâ€‘2â€‘miniÂ 6Â LÂ Ã—Â 8Â HÂ Ã—Â 256Â DÂ (â‰ˆÂ 30â€¯M params) |
| Training | 3Â epochsÂ Â· batchÂ sizeÂ 16Â (grad accumÂ 4)Â Â·Â fp16 |
| Cost | â‰ˆÂ 2Â GPUâ€‘hoursÂ (TeslaÂ T4) |
| Metrics | lossÂ â‰ˆÂ 2.8Â Â·Â pplÂ â‰ˆÂ 16.7 |

---

## 2Â Â·Â QuickÂ Start
```bash
# clone
git clone https://github.com/<YOUR_GH_USER>/tiny-transformer-tr.git && cd tiny-transformer-tr

# install
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# train (local CUDA)
python train.py --epochs 3 --batch 4 --mixed_precision fp16
```
Or hit the **Colab** badge above and run every cellÂ â€“ free GPU, zero setup.

---

## 3Â Â·Â DataÂ Pipeline
```
Wikipedia dump â†’ clean_text.py â†’ data/wiki_tr.txtÂ (2Â GB)
```
Cleaning rules:
* Strip HTML, tables, refs  
* Turkish sentences only (`langdetect`Â â‰¥Â 0.9)  
* Collapse multiple spacesÂ â†’ one space  

---

## 4Â Â·Â Tokenizer
`tokenizers` **ByteLevelBPETokenizer** is trained **from scratch**:
```python
from tokenizers import ByteLevelBPETokenizer

tokenizer = ByteLevelBPETokenizer()
tokenizer.train(
    files=["data/wiki_tr.txt"],
    vocab_size=30_000,
    min_frequency=2,
    special_tokens=["<s>", "<pad>", "</s>", "<unk>", "<mask>"]
)
```
Artifacts live in `tokenizer_tr/` and can be pushed to the Hub so the community can reuse them.

---

## 5Â Â·Â ModelÂ Architecture
```
6 Ã— [Multiâ€‘HeadÂ AttentionÂ 8Â HÂ â†’Â GELUÂ â†’Â ResidualÂ +Â LayerNorm]
Â Â Â Â Â â†³ hiddenÂ sizeÂ 256 Â· biasÂ False Â· dropoutÂ 0.1
LM head ties weights with input embeddings.
```
Design decisions:
* **Context 512 tokens** â€“ fits into 14Â GB RAM with fp16.  
* **Weight tying** â€“ parameter efficient, faster convergence.  
* **No activation checkpointing** â€“ small model, not worth overhead.  

---

## 6Â Â·Â TrainingÂ Strategy
* **Optimizer**Â â€“ AdamWÂ Î²=(0.9,Â 0.95)Â Â·Â Îµ=1eâ€‘8  
* **LRÂ Scheduler**Â â€“ Linear warmâ€‘upÂ 5â€¯%Â â†’ cosine decay  
* **GradientÂ Accumulation**Â â€“ 4Â steps (effective batchÂ 16) to squeeze into T4 GPU memory.  
* **MixedÂ Precision**Â â€“ Automatic fp16 with PyTorch `amp`.  
* **Checkpointing**Â â€“ everyÂ 500Â steps; keepÂ 2Â latest.  
* **Logging**Â â€“ TensorBoard +Â W&B (optional).  

> **Compute budget:** 2â€¯hÂ (T4)Â orÂ 45â€¯minÂ (A100)Â â‰ˆÂ $0Â (Colab)Â /Â $0.5Â (spotÂ A100).

---

## 7Â Â·Â Evaluation
* Perplexity on heldâ€‘outÂ 50â€¯k sentences.  
* `evaluate.py` script provides BLEU &Â perplexity.  
* Qualitative generation samples stored under `samples/`.  

---

## 8Â Â·Â Deployment
* **Push toÂ HF**Â â€“ `python push_to_hub.py`  
* **Gradio Space**Â â€“ readyâ€‘made `app.py` gives chat UI.  
* **ONNX**Â â€“ `python export_onnx.py` converts &Â quantizesÂ â†’Â 2Ã— faster CPU inference.  
* **Docker**Â â€“ `docker build -t ttt-infer -f docker/Dockerfile .`  

---

## 9Â Â·Â ResultsÂ &Â Samples
```text
> Merhaba, bugÃ¼n hava nasÄ±l?
>>>> BugÃ¼n Ä°stanbul genelinde parÃ§alÄ± bulutlu bir gÃ¶kyÃ¼zÃ¼ bekleniyor. SÄ±caklÄ±k Ã¶ÄŸleden sonra 24â€“26â€¯Â°C aralÄ±ÄŸÄ±ndaâ€¦
```
More examples in `samples/`.

---

## 10Â Â·Â ProjectÂ Structure
```text
â”œâ”€â”€ data/                  # raw + cleaned corpora
â”œâ”€â”€ tokenizer_tr/          # BPE vocab/merges
â”œâ”€â”€ tiny-transformer-tr/   # final model snapshot
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ clean_text.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”œâ”€â”€ export_onnx.py
â”‚   â””â”€â”€ push_to_hub.py
â”œâ”€â”€ train.py               # main training entry
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## 11Â Â·Â Contributing
PRâ€™ler, issueâ€™lar &Â feature Ã¶nerileri memnuniyetle karÅŸÄ±lanÄ±r. LÃ¼tfen `pre-commit` hookâ€™larÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±n:
```bash
pre-commit install
pre-commit run --all-files
```

---

## 12Â Â·Â License
MIT â€“ Ã¶zgÃ¼rce kullan, yÄ±ldÄ±z bÄ±rakÄ±rsanÂ â­Â mutlu oluruz.

---

> Â©Â 2025Â <Deniz_Sakaroglu>Â Â·Â Made in TÃ¼rkiye withÂ ğŸ§ Â &Â â˜•
